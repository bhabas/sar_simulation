https://towardsdatascience.com/reinforcement-learning-made-simple-part-2-solution-approaches-7e37cbf2334e


RL Notes:
Model-based (aka Planning)
    - Environment is known
    - Can predict next state and reward from action and current state

Model-free (aka RL)
    - environment is complex
    - internal dynamics are unknown
    - Env is black box

Control Problem
    - No input provided
    - Goal is to explore policy space and find optimal policy

Popular RL algorithms
    - Q Learning
    - DQN
    - Policy Gradient
    - Actor-Critic
    - PPO

Our system:
    - Continuous state-space
    - Continuous action-space
    - One action per episode

Model-free algorithms can be Policy-based or Value-based:
    - Every policy has two corresponding value functions
        - State Value V(s)
        - State-Action Value Q(s,a) 
    - Finding optical policy is equivalent to finding optimal State-Action Value


Policy-based vs Value-based algorithms
    - Can find optimal policy directly or indirectly
    - State-Action Value-based (indirect)
    - Policy-based (direct)

    - Optimal policy is typically deterministic because is picks best option
    - Can be stochastic if there is a tie between multiple Q-values
        - Picks with equal probability

Lookup Table vs Function:
    - Simpler algorithms implement Policy or Value as a Lookup Table
    - More complex us Function Approximator (NN)

    Lookup:
        - Q Learning
        - Sarsa
    
    Function Approximator:
        - Policy Gradient
        - Actor Critic
        - DQN

Essential principles of RL algorithms:
    1) Start with arbitrary estimates
    2) Take Action
    3) Get feedback
    4) Improve estimates based on feedback


1) Initialize estimates:
    - Value-based algorithms use estimated Optimal State-Action Value table
    - Policy based algorithms use estimated Optimal Policy table with probabilities for each state

2) Take action:
    - All states want to be explored so used Exporation-Exploitation tradeoff